{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Lambda, Conv2D, MaxPooling2D, Dropout, Dense, Flatten\n",
    "\n",
    "import cv2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from utils import DataFrame\n",
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========\n",
      "\n",
      "csv filepath:  ./road-images-2/new-red.csv\n",
      "number of images:  7282\n",
      "sample data: \n",
      "                                                NAME  STEER\n",
      "0  ./road-images-2/red-flat/eagle_2018_09_14_20_2...    0.0\n",
      "1  ./road-images-2/red-flat/eagle_2018_09_14_20_2...    0.0\n",
      "2  ./road-images-2/red-flat/eagle_2018_09_14_20_2...    0.0\n",
      "3  ./road-images-2/red-flat/eagle_2018_09_14_20_2...    0.0\n",
      "4  ./road-images-2/red-flat/eagle_2018_09_14_20_2...    0.0\n",
      "\n",
      "=========\n",
      "\n",
      "csv filepath:  ./road-images-2/new-green.csv\n",
      "number of images:  6487\n",
      "sample data: \n",
      "                                                NAME  STEER\n",
      "0  ./road-images-2/green-flat/eagle_2018_09_14_20...    0.0\n",
      "1  ./road-images-2/green-flat/eagle_2018_09_14_20...    0.0\n",
      "2  ./road-images-2/green-flat/eagle_2018_09_14_20...    0.0\n",
      "3  ./road-images-2/green-flat/eagle_2018_09_14_20...    0.0\n",
      "4  ./road-images-2/green-flat/eagle_2018_09_14_20...    0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model Parameters \n",
    "BATCH_SIZE = 128\n",
    "NUMBER_OF_EPOCHS = 10\n",
    "VALIDATION_SIZE = 0.15\n",
    "\n",
    "#https://keras.io/optimizers/\n",
    "OPTIMIZER_TYPE = \"adam\"\n",
    "LOSS_TYPE = \"mse\"\n",
    "\n",
    "INPUT_H, INPUT_W = 75, 200\n",
    "INPUT_SHAPE = (INPUT_H, INPUT_W, 3)\n",
    "# Crop parameters\n",
    "YSTART = 110\n",
    "YSTOP = 230\n",
    "\n",
    "print(\"=========\")\n",
    "red_frame = DataFrame(\"./road-images-2/new-red.csv\")\n",
    "print(\"=========\")\n",
    "green_frame = DataFrame(\"./road-images-2/new-green.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_flip(image, steer):\n",
    "    if np.random.random() > 0.5:\n",
    "        return np.fliplr(image), -steer\n",
    "    else:\n",
    "        return image, steer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_data(x, data):\n",
    "    \n",
    "    i = data.index[x]\n",
    "    steer =  data['STEER'][i]\n",
    "    img = mpimg.imread(data['NAME'][i])\n",
    "    img = img[YSTART:YSTOP, :, :]\n",
    "    img = cv2.resize(img, (INPUT_W, INPUT_H), cv2.INTER_AREA)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n",
    "    \n",
    "    return random_flip(img, steer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(data, batch_size):\n",
    "\n",
    "    while True:\n",
    "\n",
    "        SIZE = len(data)\n",
    "        data.sample(frac=1) #shuffle\n",
    "\n",
    "        for start in range(0, SIZE, batch_size):\n",
    "            images, steers = [], []\n",
    "\n",
    "            for i in range(start, start + batch_size):\n",
    "                if i < SIZE:\n",
    "                    image, steer = get_processed_data(i, data)\n",
    "                    steers.append(steer)\n",
    "                    images.append(image)\n",
    "\n",
    "            yield (np.array(images), np.array(steers))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_modified_nvidia_model():\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x: x/127.5-1.0, input_shape=INPUT_SHAPE))\n",
    "    model.add(Conv2D(24, (5, 5), activation='elu', strides=(2, 2)))\n",
    "    model.add(Conv2D(36, (5, 5), activation='elu', strides=(2, 2)))\n",
    "    model.add(Conv2D(48, (5, 5), activation='elu', strides=(2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='elu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='elu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='elu'))\n",
    "    model.add(Dense(50, activation='elu'))\n",
    "    model.add(Dense(10, activation='elu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=OPTIMIZER_TYPE, loss=LOSS_TYPE)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data, modelh5_name, modeljson_name):\n",
    "    \n",
    "    print('summary of model:')\n",
    "    model.summary()\n",
    "    \n",
    "\n",
    "    TRAINING_DATA, VALIDATION_DATA = train_test_split(data, test_size=VALIDATION_SIZE)\n",
    "    TOTAL_TRAIN_DATA = len(TRAINING_DATA)\n",
    "    TOTAL_VALID_DATA = len(VALIDATION_DATA)\n",
    "    \n",
    "    print(\"Total training data:\", TOTAL_TRAIN_DATA)\n",
    "    print(\"Total validation data:\", TOTAL_VALID_DATA)\n",
    "\n",
    "    print('Training model...')\n",
    "    train_generator = generate_samples(TRAINING_DATA, batch_size=BATCH_SIZE)\n",
    "    validation_generator = generate_samples(VALIDATION_DATA, batch_size=BATCH_SIZE)\n",
    "\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        validation_data=validation_generator,         \n",
    "        steps_per_epoch=TOTAL_TRAIN_DATA // BATCH_SIZE,\n",
    "        validation_steps=TOTAL_VALID_DATA // BATCH_SIZE,\n",
    "        epochs=NUMBER_OF_EPOCHS,\n",
    "        #verbose=1,\n",
    "    )\n",
    "    \n",
    "    print('...Model trained.')\n",
    "    print('Saving model...')\n",
    "    model.save(modelh5_name)\n",
    "\n",
    "    with open(modeljson_name, \"w\") as json_file:\n",
    "        json_file.write(model.to_json())\n",
    "\n",
    "    print(\"...Model Saved at:\", modelh5_name, \"and\", modeljson_name, \".\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_model = build_modified_nvidia_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary of model:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_1 (Lambda)            (None, 75, 200, 3)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 36, 98, 24)        1824      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 47, 36)        21636     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 6, 22, 48)         43248     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 20, 64)         27712     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 2, 18, 64)         36928     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2, 18, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               230500    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 367,419\n",
      "Trainable params: 367,419\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Total training data: 6189\n",
      "Total validation data: 1093\n",
      "Training model...\n",
      "Epoch 1/10\n",
      "48/48 [==============================] - 86s 2s/step - loss: 45.3878 - val_loss: 36.4660\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 70s 1s/step - loss: 36.6091 - val_loss: 34.6967\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 71s 1s/step - loss: 35.3160 - val_loss: 35.6220\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 70s 1s/step - loss: 34.5406 - val_loss: 34.5442\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 72s 1s/step - loss: 34.0009 - val_loss: 36.4595\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 76s 2s/step - loss: 34.1041 - val_loss: 33.8883\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 97s 2s/step - loss: 32.3964 - val_loss: 30.0928\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 94s 2s/step - loss: 32.2371 - val_loss: 31.1098\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 88s 2s/step - loss: 31.2918 - val_loss: 31.5784\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 95s 2s/step - loss: 32.6265 - val_loss: 32.9021\n",
      "...Model trained.\n",
      "Saving model...\n",
      "...Model Saved at: ./models/red2.h5 and ./models/red2.json .\n"
     ]
    }
   ],
   "source": [
    "red_model = train_model(red_model, red_frame.data, \"./models/red2.h5\", \"./models/red2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary of model:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_2 (Lambda)            (None, 75, 200, 3)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 36, 98, 24)        1824      \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 16, 47, 36)        21636     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 6, 22, 48)         43248     \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 4, 20, 64)         27712     \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 2, 18, 64)         36928     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2, 18, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               230500    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 367,419\n",
      "Trainable params: 367,419\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Total training data: 5513\n",
      "Total validation data: 974\n",
      "Training model...\n",
      "Epoch 1/10\n",
      "39/43 [==========================>...] - ETA: 10s - loss: 39.7633"
     ]
    }
   ],
   "source": [
    "green_model = build_modified_nvidia_model()\n",
    "green_model = train_model(green_model, green_frame.data, \"./models/green2.h5\", \"./models/green2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
